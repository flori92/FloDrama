#!/bin/bash
# Script de lancement complet du scraping FloDrama
# Ce script lance le scraping, analyse les rÃ©sultats et surveille la santÃ© des sources

# Couleurs pour les messages
ROUGE='\033[0;31m'
VERT='\033[0;32m'
JAUNE='\033[0;33m'
BLEU='\033[0;34m'
NC='\033[0m' # No Color

# Dossier courant
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# Fonction pour afficher un message avec timestamp
log() {
  echo -e "${BLEU}[$(date +"%Y-%m-%d %H:%M:%S")]${NC} $1"
}

# Fonction pour afficher une section
section() {
  echo ""
  echo -e "${JAUNE}========== $1 ==========${NC}"
  echo ""
}

# VÃ©rifier si Node.js est installÃ©
if ! command -v node &> /dev/null; then
  echo -e "${ROUGE}âŒ Node.js n'est pas installÃ©. Veuillez l'installer pour continuer.${NC}"
  exit 1
fi

# CrÃ©er les dossiers nÃ©cessaires
mkdir -p "$DIR/output"
mkdir -p "$DIR/reports"
mkdir -p "$DIR/source-health"

# Fonction pour libÃ©rer le port du serveur relay
liberer_port() {
  PORT=$1
  echo -e "${JAUNE}âš ï¸ VÃ©rification du port $PORT...${NC}"
  
  # VÃ©rifier si le port est utilisÃ©
  if lsof -Pi :$PORT -sTCP:LISTEN -t >/dev/null ; then
    echo -e "${ROUGE}âš ï¸ Le port $PORT est utilisÃ©. Tentative de libÃ©ration...${NC}"
    
    # Identifier tous les processus utilisant ce port
    PIDS=$(lsof -Pi :$PORT -sTCP:LISTEN -t)
    
    # Tuer chaque processus individuellement
    for PID in $PIDS; do
      echo -e "${JAUNE}ğŸ”„ ArrÃªt du processus $PID...${NC}"
      kill -15 $PID 2>/dev/null || true
      sleep 1
      
      # Si processus toujours en vie, force kill
      if ps -p $PID > /dev/null; then
        echo -e "${ROUGE}âš ï¸ Processus $PID rÃ©sistant, force kill...${NC}"
        kill -9 $PID 2>/dev/null || true
        sleep 1
      fi
    done
    
    # VÃ©rifier que le port est libÃ©rÃ©
    if lsof -Pi :$PORT -sTCP:LISTEN -t >/dev/null ; then
      echo -e "${ROUGE}âŒ Ã‰chec de libÃ©ration du port $PORT aprÃ¨s plusieurs tentatives.${NC}"
    else
      echo -e "${VERT}âœ… Port $PORT libÃ©rÃ© avec succÃ¨s${NC}"
      return 0
    fi
  else
    echo -e "${VERT}âœ… Port $PORT disponible${NC}"
    return 0
  fi
}

# LibÃ©rer le port 3000 utilisÃ© par le serveur relay
PORT=3000
liberer_port $PORT

# DÃ©marrer le serveur relay en arriÃ¨re-plan
section "DÃ‰MARRAGE DU SERVEUR RELAY"
log "ğŸš€ DÃ©marrage du serveur relay local..."

# Sauvegarder le PID dans un fichier pour pouvoir l'arrÃªter plus tard
node "$DIR/serveur-relay-local-v2.js" > "$DIR/relay-logs.txt" 2>&1 &
RELAY_PID=$!
echo $RELAY_PID > "$DIR/relay_pid.txt"
echo "running" > "$DIR/relay_status.txt"

log "âœ… Serveur relay dÃ©marrÃ© avec PID: $RELAY_PID"
log "ğŸ“ Logs disponibles dans: $DIR/relay-logs.txt"

# Attendre que le serveur soit prÃªt et vÃ©rifier qu'il fonctionne correctement
log "â³ Attente du dÃ©marrage complet du serveur relay..."
sleep 5

# VÃ©rifier que le serveur est bien en cours d'exÃ©cution
if ! ps -p $RELAY_PID > /dev/null; then
  echo -e "${ROUGE}âŒ Le serveur relay n'a pas dÃ©marrÃ© correctement. VÃ©rifiez les logs pour plus de dÃ©tails.${NC}"
  cat "$DIR/relay-logs.txt"
  exit 1
fi

# Lancer le scraping
section "LANCEMENT DU SCRAPING GÃ‰NÃ‰RAL"
log "ğŸ” DÃ©marrage du scraping sur toutes les sources..."

node "$DIR/test-scraping-local.js"

log "âœ… Scraping gÃ©nÃ©ral terminÃ©"

# Fonction pour extraire les URLs de streaming d'une source
extraire_streaming() {
  SOURCE=$1
  LIMIT=$2
  
  log "ğŸ“º Extraction depuis $SOURCE (limite: $LIMIT)..."
  
  # VÃ©rifier que la source est configurÃ©e
  if ! grep -q "'$SOURCE'" "$DIR/src/sources-config.js"; then
    echo -e "${ROUGE}âš ï¸ Source '$SOURCE' non trouvÃ©e dans la configuration!${NC}"
    return 1
  fi
  
  # Extraire les URLs de streaming avec limitation de temps (macOS compatible)
  # DÃ©finir la durÃ©e maximale en secondes
  MAX_DURATION=300
  
  # Lancer l'extraction en arriÃ¨re-plan
  node "$DIR/src/enhanced-streaming-extractor.js" --source "$SOURCE" --limit "$LIMIT" & 
  EXTRACT_PID=$!
  
  # Attendre le processus avec timeout personnalisÃ©
  SECONDS=0
  while kill -0 $EXTRACT_PID 2>/dev/null; do
    if [ $SECONDS -ge $MAX_DURATION ]; then
      echo -e "${ROUGE}âš ï¸ DurÃ©e maximale de $MAX_DURATION secondes atteinte, arrÃªt forcÃ©...${NC}"
      kill -9 $EXTRACT_PID 2>/dev/null
      wait $EXTRACT_PID 2>/dev/null
      return 124  # Code de retour similaire Ã  timeout
    fi
    # Attendre 1 seconde
    sleep 1
  done
  
  # RÃ©cupÃ©rer le code de retour du processus
  wait $EXTRACT_PID
  RESULT=$?
  
  # VÃ©rifier le code de retour
  if [ $RESULT -eq 124 ]; then
    echo -e "${ROUGE}âš ï¸ Timeout atteint lors de l'extraction depuis $SOURCE${NC}"
    return 1
  elif [ $RESULT -ne 0 ]; then
    echo -e "${ROUGE}âš ï¸ Erreur lors de l'extraction depuis $SOURCE (code: $RESULT)${NC}"
    return $RESULT
  fi
  
  # VÃ©rifier les rÃ©sultats
  COUNT=$(ls -1 "$DIR/scraping-results/streaming/${SOURCE}_"* 2>/dev/null | wc -l)
  if [ $COUNT -eq 0 ]; then
    echo -e "${JAUNE}âš ï¸ Aucune URL de streaming extraite depuis $SOURCE${NC}"
    return 0
  else
    echo -e "${VERT}âœ… $COUNT URL(s) de streaming extraite(s) depuis $SOURCE${NC}"
    return 0
  fi
}

# Lancer l'extraction des streaming
section "EXTRACTION DES URLS DE STREAMING"
log "ğŸ¬ DÃ©marrage de l'extraction des URLs de streaming..."

# CrÃ©ation du dossier pour les rÃ©sultats de streaming s'il n'existe pas
mkdir -p "$DIR/scraping-results/streaming"

# DÃ©finir la liste des sources et la limite d'extraction par catÃ©gorie
# Dramas
DRAMAS_SOURCES=("dramacool" "viewasian" "kissasian" "voirdrama")
# Animes
ANIMES_SOURCES=("gogoanime" "nekosama" "voiranime")
# Films
FILMS_SOURCES=("vostfree" "streamingdivx" "filmcomplet")
# Bollywood
BOLLYWOOD_SOURCES=("bollyplay" "hindilinks4u")

# Combiner toutes les sources
SOURCES=(${DRAMAS_SOURCES[@]} ${ANIMES_SOURCES[@]} ${FILMS_SOURCES[@]} ${BOLLYWOOD_SOURCES[@]})
LIMIT=10
SUCCESS_COUNT=0
FAIL_COUNT=0

# Extraire les URLs de streaming pour chaque source
for SOURCE in "${SOURCES[@]}"; do
  extraire_streaming "$SOURCE" "$LIMIT"
  if [ $? -eq 0 ]; then
    SUCCESS_COUNT=$((SUCCESS_COUNT+1))
  else
    FAIL_COUNT=$((FAIL_COUNT+1))
  fi
done

# RÃ©capitulatif
section "RÃ‰CAPITULATIF DE L'EXTRACTION"
log "ğŸ“Š Bilan de l'extraction des URLs de streaming:"
log "  - Sources traitÃ©es avec succÃ¨s: $SUCCESS_COUNT/${#SOURCES[@]}"
log "  - Sources en Ã©chec: $FAIL_COUNT/${#SOURCES[@]}"
log "  - Dossier des rÃ©sultats: $DIR/scraping-results/streaming"

log "âœ… Extraction des URLs de streaming terminÃ©e"

# Analyser les rÃ©sultats
section "ANALYSE DES RÃ‰SULTATS"
log "ğŸ“Š Analyse des rÃ©sultats du scraping..."

node "$DIR/analyze-scraping-results.js"

# Surveiller la santÃ© des sources
section "SURVEILLANCE DE LA SANTÃ‰ DES SOURCES"
log "ğŸ¥ VÃ©rification de la santÃ© des sources..."

node "$DIR/monitor-sources-health.js"

# ArrÃªter le serveur relay
section "ARRÃŠT DU SERVEUR RELAY"
log "ğŸ›‘ ArrÃªt du serveur relay local..."

if [ -f "$DIR/relay_pid.txt" ]; then
  RELAY_PID=$(cat "$DIR/relay_pid.txt")
  if ps -p $RELAY_PID > /dev/null; then
    kill $RELAY_PID || true
    sleep 1
    # VÃ©rifier si le processus est toujours en cours d'exÃ©cution
    if ps -p $RELAY_PID > /dev/null; then
      log "âš ï¸ Le processus ne s'est pas arrÃªtÃ© normalement, utilisation de kill -9"
      kill -9 $RELAY_PID || true
    fi
  else
    log "âš ï¸ Le processus $RELAY_PID n'est plus en cours d'exÃ©cution"
  fi
  rm "$DIR/relay_pid.txt"
  echo "stopped" > "$DIR/relay_status.txt"
  log "âœ… Serveur relay arrÃªtÃ©"
else
  log "âš ï¸ Fichier PID non trouvÃ©, recherche des processus node en cours..."
  # Rechercher les processus node qui pourraient Ãªtre le serveur relay
  NODE_PIDS=$(ps aux | grep "[n]ode.*serveur-relay" | awk '{print $2}')
  if [ -n "$NODE_PIDS" ]; then
    log "ğŸ” Processus serveur relay trouvÃ©s: $NODE_PIDS"
    for PID in $NODE_PIDS; do
      kill $PID || true
    done
    log "âœ… Processus arrÃªtÃ©s"
  else
    log "âœ… Aucun processus serveur relay en cours d'exÃ©cution"
  fi
fi

# RÃ©sumÃ© final
section "RÃ‰SUMÃ‰"
log "âœ… Processus de scraping complet terminÃ©"
log "ğŸ“ RÃ©sultats disponibles dans:"
log "   - DonnÃ©es: $DIR/output/"
log "   - Rapports: $DIR/reports/"
log "   - SantÃ© des sources: $DIR/source-health/"

echo ""
log "${VERT}Merci d'avoir utilisÃ© le systÃ¨me de scraping FloDrama !${NC}"
echo ""
