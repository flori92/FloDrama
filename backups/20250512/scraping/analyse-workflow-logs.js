/**
 * Script d'analyse des logs de workflow GitHub Actions pour FloDrama
 * 
 * Ce script permet d'analyser les logs des workflows GitHub Actions
 * pour identifier les probl√®mes potentiels dans le processus de scraping.
 * 
 * @author FloDrama Team
 * @version 1.0.0
 */

const fs = require('fs-extra');
const path = require('path');
const axios = require('axios');
const readline = require('readline');

// Configuration
const LOG_DIR = path.join(__dirname, 'workflow-logs');
const ANALYSIS_FILE = path.join(LOG_DIR, 'analyse-resultats.md');
const GITHUB_REPO = 'flori92/FloDrama';

// Motifs d'erreurs courants √† rechercher
const ERROR_PATTERNS = [
  {
    pattern: /Navigation timeout of \d+ ms exceeded/i,
    type: 'timeout',
    description: 'Timeout de navigation',
    suggestion: 'Augmenter le timeout de navigation ou optimiser le s√©lecteur d\'attente'
  },
  {
    pattern: /waiting for selector `([^`]+)` failed/i,
    type: 'selector',
    description: 'S√©lecteur introuvable',
    suggestion: 'V√©rifier que le s√©lecteur existe toujours sur la page ou le mettre √† jour'
  },
  {
    pattern: /ERR_PROXY_CONNECTION_FAILED/i,
    type: 'proxy',
    description: '√âchec de connexion au proxy',
    suggestion: 'V√©rifier la configuration du proxy ou utiliser le service relais Render'
  },
  {
    pattern: /net::ERR_NAME_NOT_RESOLVED/i,
    type: 'dns',
    description: 'Erreur de r√©solution DNS',
    suggestion: 'V√©rifier la connectivit√© r√©seau ou utiliser le service relais Render'
  },
  {
    pattern: /net::ERR_CONNECTION_REFUSED/i,
    type: 'connection',
    description: 'Connexion refus√©e',
    suggestion: 'Le site cible bloque peut-√™tre les requ√™tes, utiliser le service relais Render'
  },
  {
    pattern: /captcha|cloudflare|challenge/i,
    type: 'protection',
    description: 'Protection anti-bot d√©tect√©e',
    suggestion: 'Utiliser le service relais Render qui contourne ces protections'
  },
  {
    pattern: /cannot read property .* of (undefined|null)/i,
    type: 'data',
    description: 'Donn√©es manquantes ou structure modifi√©e',
    suggestion: 'Mettre √† jour les s√©lecteurs ou la logique d\'extraction des donn√©es'
  },
  {
    pattern: /TypeError: (.*)/i,
    type: 'type',
    description: 'Erreur de type',
    suggestion: 'V√©rifier les types de donn√©es et ajouter des v√©rifications'
  },
  {
    pattern: /ECONNRESET|ETIMEDOUT|ESOCKETTIMEDOUT/i,
    type: 'network',
    description: 'Erreur r√©seau',
    suggestion: 'Ajouter des m√©canismes de retry ou utiliser le service relais Render'
  }
];

// Fonction pour t√©l√©charger les logs des workflows r√©cents
async function downloadWorkflowLogs() {
  console.log('üì• T√©l√©chargement des logs des workflows r√©cents...');
  
  try {
    // Cr√©er le dossier de logs s'il n'existe pas
    fs.ensureDirSync(LOG_DIR);
    
    // Si nous sommes dans un environnement GitHub Actions avec un token
    if (process.env.GITHUB_TOKEN) {
      const octokit = require('@octokit/rest')({
        auth: process.env.GITHUB_TOKEN
      });
      
      // R√©cup√©rer les ex√©cutions de workflow r√©centes
      const [owner, repo] = GITHUB_REPO.split('/');
      const { data } = await octokit.actions.listWorkflowRunsForRepo({
        owner,
        repo,
        per_page: 5
      });
      
      // T√©l√©charger les logs pour chaque ex√©cution
      for (const run of data.workflow_runs) {
        if (run.name.includes('Scraping')) {
          console.log(`T√©l√©chargement des logs pour l'ex√©cution #${run.id} (${run.created_at})...`);
          
          try {
            const logsResponse = await octokit.actions.downloadWorkflowRunLogs({
              owner,
              repo,
              run_id: run.id
            });
            
            // Sauvegarder les logs
            const logFile = path.join(LOG_DIR, `workflow-${run.id}.zip`);
            fs.writeFileSync(logFile, Buffer.from(logsResponse.data));
            console.log(`‚úÖ Logs sauvegard√©s dans ${logFile}`);
            
            // Extraire les logs
            const extract = require('extract-zip');
            await extract(logFile, { dir: path.join(LOG_DIR, `run-${run.id}`) });
          } catch (error) {
            console.warn(`‚ö†Ô∏è Impossible de t√©l√©charger les logs pour l'ex√©cution #${run.id}: ${error.message}`);
          }
        }
      }
    } else {
      console.log('‚ö†Ô∏è Token GitHub non disponible, utilisation des logs locaux uniquement');
    }
    
    return true;
  } catch (error) {
    console.error(`‚ùå Erreur lors du t√©l√©chargement des logs: ${error.message}`);
    return false;
  }
}

// Fonction pour analyser un fichier de log
async function analyzeLogFile(filePath) {
  console.log(`üîç Analyse du fichier de log: ${filePath}`);
  
  const errors = [];
  const warnings = [];
  const stats = {
    totalLines: 0,
    errorLines: 0,
    warningLines: 0,
    sources: {},
    startTime: null,
    endTime: null
  };
  
  // Cr√©er une interface de lecture de ligne
  const fileStream = fs.createReadStream(filePath);
  const rl = readline.createInterface({
    input: fileStream,
    crlfDelay: Infinity
  });
  
  // Parcourir le fichier ligne par ligne
  for await (const line of rl) {
    stats.totalLines++;
    
    // Extraire l'horodatage si pr√©sent
    const timestampMatch = line.match(/\[(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{3}Z)\]/);
    if (timestampMatch) {
      const timestamp = new Date(timestampMatch[1]);
      if (!stats.startTime || timestamp < stats.startTime) {
        stats.startTime = timestamp;
      }
      if (!stats.endTime || timestamp > stats.endTime) {
        stats.endTime = timestamp;
      }
    }
    
    // D√©tecter les sources
    const sourceMatch = line.match(/Source: ([a-z0-9-]+)/i);
    if (sourceMatch) {
      const source = sourceMatch[1];
      if (!stats.sources[source]) {
        stats.sources[source] = {
          items: 0,
          errors: 0,
          warnings: 0
        };
      }
    }
    
    // D√©tecter les √©l√©ments r√©cup√©r√©s
    const itemsMatch = line.match(/R√©cup√©r√© (\d+) √©l√©ments? pour ([a-z0-9-]+)/i);
    if (itemsMatch) {
      const count = parseInt(itemsMatch[1]);
      const source = itemsMatch[2];
      if (stats.sources[source]) {
        stats.sources[source].items += count;
      }
    }
    
    // D√©tecter les erreurs connues
    for (const errorPattern of ERROR_PATTERNS) {
      const match = line.match(errorPattern.pattern);
      if (match) {
        stats.errorLines++;
        
        // Extraire la source concern√©e si possible
        let source = 'inconnue';
        const sourceContext = line.match(/Source: ([a-z0-9-]+)/i);
        if (sourceContext) {
          source = sourceContext[1];
          if (stats.sources[source]) {
            stats.sources[source].errors++;
          }
        }
        
        errors.push({
          line: stats.totalLines,
          type: errorPattern.type,
          description: errorPattern.description,
          suggestion: errorPattern.suggestion,
          source,
          text: line
        });
        
        break;
      }
    }
    
    // D√©tecter les avertissements
    if (line.includes('‚ö†Ô∏è') || line.toLowerCase().includes('warning')) {
      stats.warningLines++;
      
      // Extraire la source concern√©e si possible
      let source = 'inconnue';
      const sourceContext = line.match(/Source: ([a-z0-9-]+)/i);
      if (sourceContext) {
        source = sourceContext[1];
        if (stats.sources[source]) {
          stats.sources[source].warnings++;
        }
      }
      
      warnings.push({
        line: stats.totalLines,
        source,
        text: line
      });
    }
  }
  
  return {
    filePath,
    stats,
    errors,
    warnings
  };
}

// Fonction pour g√©n√©rer un rapport d'analyse
function generateAnalysisReport(analysisResults) {
  console.log('üìù G√©n√©ration du rapport d\'analyse...');
  
  let report = `# Rapport d'analyse des logs de workflow FloDrama\n\n`;
  report += `*G√©n√©r√© le ${new Date().toLocaleString('fr-FR')}*\n\n`;
  
  // R√©sum√© global
  report += `## R√©sum√© global\n\n`;
  
  let totalErrors = 0;
  let totalWarnings = 0;
  let totalItems = 0;
  const sourcesStats = {};
  
  analysisResults.forEach(result => {
    totalErrors += result.errors.length;
    totalWarnings += result.warnings.length;
    
    // Agr√©ger les statistiques par source
    Object.entries(result.stats.sources).forEach(([source, stats]) => {
      if (!sourcesStats[source]) {
        sourcesStats[source] = { items: 0, errors: 0, warnings: 0 };
      }
      sourcesStats[source].items += stats.items;
      sourcesStats[source].errors += stats.errors;
      sourcesStats[source].warnings += stats.warnings;
      totalItems += stats.items;
    });
  });
  
  report += `- **Fichiers analys√©s**: ${analysisResults.length}\n`;
  report += `- **Erreurs d√©tect√©es**: ${totalErrors}\n`;
  report += `- **Avertissements**: ${totalWarnings}\n`;
  report += `- **√âl√©ments r√©cup√©r√©s**: ${totalItems}\n\n`;
  
  // Tableau des sources
  report += `### Performance par source\n\n`;
  report += `| Source | √âl√©ments | Erreurs | Avertissements | Taux de succ√®s |\n`;
  report += `|--------|----------|---------|----------------|---------------|\n`;
  
  Object.entries(sourcesStats).forEach(([source, stats]) => {
    const successRate = stats.errors > 0 ? 
      Math.round((1 - stats.errors / (stats.items > 0 ? stats.items : 1)) * 100) : 
      (stats.items > 0 ? 100 : 0);
    
    report += `| ${source} | ${stats.items} | ${stats.errors} | ${stats.warnings} | ${successRate}% |\n`;
  });
  
  report += `\n`;
  
  // Analyse des erreurs par type
  report += `## Types d'erreurs fr√©quentes\n\n`;
  
  const errorTypes = {};
  analysisResults.forEach(result => {
    result.errors.forEach(error => {
      if (!errorTypes[error.type]) {
        errorTypes[error.type] = {
          count: 0,
          description: error.description,
          suggestion: error.suggestion,
          examples: []
        };
      }
      errorTypes[error.type].count++;
      
      // Ajouter un exemple si nous n'en avons pas trop
      if (errorTypes[error.type].examples.length < 3) {
        errorTypes[error.type].examples.push({
          source: error.source,
          text: error.text
        });
      }
    });
  });
  
  // Trier par fr√©quence
  const sortedErrorTypes = Object.entries(errorTypes)
    .sort((a, b) => b[1].count - a[1].count)
    .map(([type, data]) => ({ type, ...data }));
  
  sortedErrorTypes.forEach(error => {
    report += `### ${error.description} (${error.count} occurrences)\n\n`;
    report += `**Suggestion**: ${error.suggestion}\n\n`;
    
    if (error.examples.length > 0) {
      report += `**Exemples**:\n\n`;
      error.examples.forEach(example => {
        report += `- Source: ${example.source}\n`;
        report += `  \`\`\`\n  ${example.text}\n  \`\`\`\n`;
      });
    }
    
    report += `\n`;
  });
  
  // Analyse d√©taill√©e par fichier
  report += `## Analyse d√©taill√©e par fichier\n\n`;
  
  analysisResults.forEach(result => {
    const fileName = path.basename(result.filePath);
    const duration = result.stats.endTime && result.stats.startTime ? 
      ((result.stats.endTime - result.stats.startTime) / 1000 / 60).toFixed(2) : 
      'N/A';
    
    report += `### ${fileName}\n\n`;
    report += `- **Dur√©e**: ${duration} minutes\n`;
    report += `- **Lignes totales**: ${result.stats.totalLines}\n`;
    report += `- **Erreurs**: ${result.errors.length}\n`;
    report += `- **Avertissements**: ${result.warnings.length}\n\n`;
    
    if (result.errors.length > 0) {
      report += `#### Erreurs\n\n`;
      result.errors.slice(0, 10).forEach(error => {
        report += `- Ligne ${error.line} (${error.type}): ${error.text.substring(0, 100)}...\n`;
      });
      
      if (result.errors.length > 10) {
        report += `- ... et ${result.errors.length - 10} autres erreurs\n`;
      }
      
      report += `\n`;
    }
  });
  
  // Recommandations
  report += `## Recommandations\n\n`;
  
  // D√©terminer les recommandations bas√©es sur les erreurs les plus fr√©quentes
  if (sortedErrorTypes.length > 0) {
    const topError = sortedErrorTypes[0];
    
    if (topError.type === 'protection') {
      report += `1. **Utiliser le service relais Render**: La majorit√© des erreurs sont li√©es √† des protections anti-bot. Le service relais Render est sp√©cialement con√ßu pour contourner ces protections.\n\n`;
      report += `2. **Mettre √† jour les s√©lecteurs**: Certains s√©lecteurs semblent ne plus fonctionner, ce qui sugg√®re que les sites cibles ont √©t√© mis √† jour.\n\n`;
    } else if (topError.type === 'timeout' || topError.type === 'network') {
      report += `1. **Augmenter les timeouts**: De nombreuses erreurs sont li√©es √† des timeouts. Augmenter les valeurs de timeout pourrait aider.\n\n`;
      report += `2. **Impl√©menter un m√©canisme de retry**: Ajouter des tentatives automatiques en cas d'√©chec r√©seau.\n\n`;
      report += `3. **Utiliser le service relais Render**: Ce service dispose d'une meilleure connectivit√© et stabilit√©.\n\n`;
    } else if (topError.type === 'selector' || topError.type === 'data') {
      report += `1. **Mettre √† jour les s√©lecteurs**: Les sites cibles ont probablement modifi√© leur structure HTML.\n\n`;
      report += `2. **Ajouter plus de v√©rifications**: V√©rifier l'existence des √©l√©ments avant d'essayer d'y acc√©der.\n\n`;
      report += `3. **Utiliser des s√©lecteurs plus robustes**: Pr√©f√©rer des s√©lecteurs bas√©s sur des attributs data-* plut√¥t que des classes qui changent souvent.\n\n`;
    }
  }
  
  report += `### Recommandation g√©n√©rale\n\n`;
  report += `Bas√© sur l'analyse des logs, nous recommandons d'utiliser principalement le service relais Render pour le scraping, avec un fallback vers TMDB pour l'enrichissement des donn√©es. Cette approche hybride devrait am√©liorer significativement la fiabilit√© du pipeline de scraping.\n\n`;
  
  return report;
}

// Fonction principale
async function analyzeWorkflowLogs() {
  console.log('üîç Analyse des logs de workflow GitHub Actions');
  console.log('================================================================================');
  
  try {
    // T√©l√©charger les logs r√©cents si possible
    await downloadWorkflowLogs();
    
    // Trouver tous les fichiers de log √† analyser
    const logFiles = [];
    
    // Chercher dans le dossier principal
    const dirFiles = fs.readdirSync(LOG_DIR);
    dirFiles.forEach(file => {
      if (file.endsWith('.log')) {
        logFiles.push(path.join(LOG_DIR, file));
      }
    });
    
    // Chercher dans les sous-dossiers d'ex√©cution
    const runDirs = dirFiles.filter(file => file.startsWith('run-'));
    runDirs.forEach(runDir => {
      const runPath = path.join(LOG_DIR, runDir);
      if (fs.statSync(runPath).isDirectory()) {
        const runFiles = fs.readdirSync(runPath);
        runFiles.forEach(file => {
          if (file.includes('scraping') && file.endsWith('.txt')) {
            logFiles.push(path.join(runPath, file));
          }
        });
      }
    });
    
    // Si aucun fichier n'est trouv√©, cr√©er un exemple
    if (logFiles.length === 0) {
      console.log('‚ö†Ô∏è Aucun fichier de log trouv√©, cr√©ation d\'un exemple...');
      
      const exampleLogDir = path.join(LOG_DIR, 'example');
      fs.ensureDirSync(exampleLogDir);
      
      const exampleLogFile = path.join(exampleLogDir, 'example-workflow.log');
      fs.writeFileSync(exampleLogFile, `
[2023-05-01T12:00:00.000Z] üîç D√©marrage du scraping...
[2023-05-01T12:00:01.000Z] Source: allocine-films
[2023-05-01T12:00:10.000Z] ‚ö†Ô∏è Warning: Slow response from server
[2023-05-01T12:00:20.000Z] ‚ùå Error: Navigation timeout of 30000 ms exceeded
[2023-05-01T12:00:21.000Z] Source: allocine-series
[2023-05-01T12:00:30.000Z] R√©cup√©r√© 5 √©l√©ments pour allocine-series
[2023-05-01T12:00:31.000Z] Source: senscritique-films
[2023-05-01T12:00:40.000Z] ‚ùå Error: waiting for selector \`.sc-e24fcf0d-5\` failed: timeout 30000ms exceeded
[2023-05-01T12:00:41.000Z] Source: tmdb-films
[2023-05-01T12:00:50.000Z] R√©cup√©r√© 20 √©l√©ments pour tmdb-films
[2023-05-01T12:01:00.000Z] ‚úÖ Scraping termin√© avec succ√®s
      `);
      
      logFiles.push(exampleLogFile);
    }
    
    console.log(`üìã Analyse de ${logFiles.length} fichiers de log...`);
    
    // Analyser chaque fichier de log
    const analysisResults = [];
    for (const logFile of logFiles) {
      const result = await analyzeLogFile(logFile);
      analysisResults.push(result);
    }
    
    // G√©n√©rer le rapport d'analyse
    const report = generateAnalysisReport(analysisResults);
    fs.writeFileSync(ANALYSIS_FILE, report);
    
    console.log(`‚úÖ Rapport d'analyse g√©n√©r√©: ${ANALYSIS_FILE}`);
    console.log('================================================================================');
    
    return true;
  } catch (error) {
    console.error(`‚ùå Erreur lors de l'analyse des logs: ${error.message}`);
    return false;
  }
}

// Ex√©cution de la fonction principale
analyzeWorkflowLogs().catch(error => {
  console.error(`‚ùå Erreur fatale: ${error.message}`);
  process.exit(1);
});
