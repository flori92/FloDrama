name: Pipeline de Scraping FloDrama (Local)

on:
  # Ex√©cution manuelle avec options configurables
  workflow_dispatch:
    inputs:
      skip_scraping:
        description: 'Sauter l''√©tape de scraping'
        required: false
        default: false
        type: boolean
      skip_enrichment:
        description: 'Sauter l''√©tape d''enrichissement'
        required: false
        default: false
        type: boolean
      skip_distribution:
        description: 'Sauter l''√©tape de distribution'
        required: false
        default: false
        type: boolean
      debug_mode:
        description: 'Activer le mode debug'
        required: false
        default: false
        type: boolean
      use_relay_service:
        description: 'Utiliser le service relais local pour le scraping'
        required: false
        default: true
        type: boolean
      sources:
        description: 'Sources √† scraper (s√©par√©es par des virgules, vide = toutes)'
        required: false
        type: string
  
  # Ex√©cution automatique quotidienne
  schedule:
    - cron: '0 2 * * *'  # Tous les jours √† 2h du matin

jobs:
  scraping:
    name: Scraping et enrichissement des donn√©es
    runs-on: ubuntu-latest
    
    env:
      TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      RENDER_API_KEY: ${{ secrets.RENDER_API_KEY || 'rnd_DJfpQC9gEu4KgTRvX8iQzMXxrteP' }}
      RENDER_SERVICE_URL: 'http://localhost:3000'
      USE_RELAY_SERVICE: ${{ github.event.inputs.use_relay_service != 'false' }}
      DEBUG_MODE: ${{ github.event.inputs.debug_mode == 'true' }}
    
    steps:
      - name: Checkout du code
        uses: actions/checkout@v3
      
      - name: Configuration de Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Installation des d√©pendances
        run: |
          # D√©sactiver les scripts d'installation automatiques
          npm install --ignore-scripts
          npm install -g wrangler
          # Installation directe des d√©pendances n√©cessaires
          npm install --no-save fs-extra axios cheerio express cors puppeteer puppeteer-extra puppeteer-extra-plugin-stealth
      
      - name: Installation de Playwright
        run: |
          npx playwright install
          npx playwright install-deps
          npx playwright install chromium
      
      # D√©marrage du serveur de relais local
      - name: D√©marrage du serveur de relais local
        if: ${{ github.event.inputs.skip_scraping != 'true' && github.event.inputs.use_relay_service != 'false' }}
        id: start_relay
        run: |
          echo "üöÄ D√©marrage du serveur de relais local..."
          node cloudflare/scraping/serveur-relay-local.js > cloudflare/scraping/relay-logs.txt 2>&1 &
          echo "relay_pid=$!" >> $GITHUB_ENV
          
          # Attendre que le serveur soit pr√™t
          sleep 5
          
          # V√©rifier que le serveur est bien d√©marr√©
          if curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer $RENDER_API_KEY" http://localhost:3000/status | grep -q "200"; then
            echo "‚úÖ Serveur de relais local d√©marr√© avec succ√®s"
            echo "relay_status=running" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Serveur de relais local non disponible, utilisation du fallback local"
            echo "relay_status=failed" >> $GITHUB_ENV
            echo "USE_RELAY_SERVICE=false" >> $GITHUB_ENV
          fi
      
      # √âtape de scraping
      - name: Ex√©cution du scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        id: scraping
        run: |
          echo "üîç D√©marrage du scraping..."
          
          # D√©terminer les sources √† scraper
          SOURCES_ARG=""
          if [ -n "${{ github.event.inputs.sources }}" ]; then
            SOURCES_ARG="--sources=${{ github.event.inputs.sources }}"
            echo "Sources sp√©cifi√©es: ${{ github.event.inputs.sources }}"
          else
            echo "Scraping de toutes les sources"
          fi
          
          # Ex√©cuter le script de scraping optimis√©
          node .github/scripts/stealth/scraper-optimise.js $SOURCES_ARG
          
          # V√©rifier si des donn√©es ont √©t√© g√©n√©r√©es
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "‚úÖ Scraping termin√© avec succ√®s"
            echo "scraping_success=true" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Le scraping n'a pas g√©n√©r√© de donn√©es, v√©rifiez les logs pour plus d'informations"
            echo "scraping_success=false" >> $GITHUB_ENV
          fi
      
      # V√©rification du statut de scraping
      - name: V√©rification du statut de scraping
        id: check_scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        run: |
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "::set-output name=success::true"
          else
            echo "::set-output name=success::false"
          fi
      
      # √âtape d'enrichissement des donn√©es
      - name: Enrichissement des donn√©es via TMDB
        if: ${{ github.event.inputs.skip_enrichment != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true') }}
        run: |
          echo "üîç D√©marrage de l'enrichissement des donn√©es..."
          node .github/scripts/enrichment/tmdb-enricher.js
          echo "‚úÖ Enrichissement termin√© avec succ√®s"
      
      # √âtape de distribution des donn√©es
      - name: Distribution des donn√©es vers Cloudflare D1
        if: ${{ github.event.inputs.skip_distribution != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true' || github.event.inputs.skip_scraping == 'true') }}
        run: |
          echo "üì¶ D√©marrage de la distribution des donn√©es..."
          
          # Cr√©er la base de donn√©es D1 si elle n'existe pas
          wrangler d1 create flodrama-content --json || echo "Base de donn√©es existe d√©j√†"
          
          # Ex√©cuter les migrations de sch√©ma
          cd cloudflare/scraping
          wrangler d1 execute flodrama-content --file=./schema.sql
          
          # Importer les donn√©es
          node ./import-to-d1.js
          
          echo "‚úÖ Distribution termin√©e avec succ√®s"
      
      # Arr√™t du serveur de relais local
      - name: Arr√™t du serveur de relais local
        if: always()
        run: |
          if [ -f "relay_status.txt" ] && [ "$(cat relay_status.txt)" = "running" ]; then
            echo "üõë Arr√™t du serveur de relais local..."
            if [ -f "relay_pid.txt" ]; then
              kill $(cat relay_pid.txt) || true
            fi
            echo "‚úÖ Serveur de relais local arr√™t√©"
          else
            echo "Aucun serveur de relais en cours d'ex√©cution"
          fi
      
      # Notification Discord
      - name: Notification Discord
        if: always()
        run: |
          STATUS="${{ job.status }}"
          COLOR="0xFF0000"  # Rouge par d√©faut (√©chec)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="0x00FF00"  # Vert (succ√®s)
            MESSAGE="‚úÖ Le pipeline de scraping s'est termin√© avec succ√®s!"
          else
            MESSAGE="‚ùå Le pipeline de scraping a √©chou√©!"
          fi
          
          # Ajouter des informations sur le mode de scraping utilis√©
          if [ "$USE_RELAY_SERVICE" == "true" ]; then
            SCRAPING_MODE="Service relais local"
          else
            SCRAPING_MODE="Scraping local + fallback TMDB"
          fi
          
          # Envoyer la notification Discord avec plus de d√©tails
          curl -H "Content-Type: application/json" \
               -d "{\"embeds\": [{\"title\": \"FloDrama Scraping\", \"description\": \"$MESSAGE\", \"color\": $COLOR, \"fields\": [{\"name\": \"Mode de scraping\", \"value\": \"$SCRAPING_MODE\", \"inline\": true}, {\"name\": \"D√©clench√© par\", \"value\": \"${{ github.event_name }}\", \"inline\": true}, {\"name\": \"Date\", \"value\": \"$(date +\"%d/%m/%Y %H:%M\")\", \"inline\": true}]}]}" \
               $DISCORD_WEBHOOK
