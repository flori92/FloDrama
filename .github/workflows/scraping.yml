name: Scraping & Sync Supabase

on:
  schedule:
    - cron: '0 3 * * *'   # Tous les jours à 3h du matin
  workflow_dispatch:

jobs:
  scrape-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set up environment variables
        run: |
          echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> $GITHUB_ENV
          echo "SUPABASE_SERVICE_KEY=${{ secrets.SUPABASE_SERVICE_KEY }}" >> $GITHUB_ENV

      - name: Run scraping scripts
        run: |
          python scraping/sources/mydramalist.py
          python scraping/sources/voiranime.py
          # Ajouter ici d'autres scripts si besoin

      - name: Commit & push new/updated data (optionnel)
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          git add .
          git commit -m " [DATA] Sync automatique données scrappées" || echo "Pas de changements à commit"
          git push || echo "Pas de push nécessaire"

      - name: Notify success (optionnel)
        if: success()
        run: echo "Scraping et sync terminés avec succès !"

      - name: Redeploy Vercel (après scraping)
        if: success()
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
        run: |
          curl -X POST "https://api.vercel.com/v1/integrations/deploy/prj_${VERCEL_PROJECT_ID}" \
            -H "Authorization: Bearer $VERCEL_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"projectId":"'${VERCEL_PROJECT_ID}'","orgId":"'${VERCEL_ORG_ID}'"}'
