name: FloDrama Scraper

on:
  schedule:
    # Exécution tous les jours à 2h du matin
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      source:
        description: 'Source à scraper (mydramalist, voiranime, voirdrama, dramavostfr, animesama)'
        required: false
        default: 'mydramalist'
      limit:
        description: "Nombre d'éléments à récupérer"
        required: false
        default: '20'
      debug:
        description: "Mode debug (affiche plus d'informations)"
        required: false
        default: true
        type: boolean

jobs:
  scrape:
    runs-on: ubuntu-latest
    name: Scraper FloDrama
    
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4
      
      - name: Configuration de Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: 'cloudflare/scraping/package.json'
      
      - name: Installation des dépendances
        run: |
          cd cloudflare/scraping
          npm install
      
      - name: Exécution du scraper
        env:
          CLOUDFLARE_WORKER_URL: ${{ secrets.CLOUDFLARE_WORKER_URL }}
        run: |
          # Création du dossier pour les résultats
          mkdir -p scraping-results
          
          # Détermination de la source à scraper
          SOURCE="${{ github.event.inputs.source || 'mydramalist' }}"
          LIMIT="${{ github.event.inputs.limit || '20' }}"
          DEBUG="${{ github.event.inputs.debug || 'true' }}"
          
          echo "Démarrage du scraping pour la source: $SOURCE avec une limite de $LIMIT éléments"
          echo "URL du Worker Cloudflare: ${CLOUDFLARE_WORKER_URL}"
          
          # Vérification de l'URL du Worker
          if [ -z "$CLOUDFLARE_WORKER_URL" ]; then
            echo "ERREUR: La variable CLOUDFLARE_WORKER_URL n'est pas définie"
            exit 1
          fi
          
          # Exécution du script de scraping avec un timeout de 5 minutes
          cd cloudflare/scraping
          timeout 300s node src/cli-scraper.js --source=$SOURCE --limit=$LIMIT --output=../../scraping-results
          
          # Vérification du code de retour
          EXIT_CODE=$?
          if [ $EXIT_CODE -eq 124 ]; then
            echo "ATTENTION: Le script a été interrompu après 5 minutes d'exécution (timeout)"
          elif [ $EXIT_CODE -ne 0 ]; then
            echo "ERREUR: Le script s'est terminé avec le code d'erreur $EXIT_CODE"
            # Ne pas échouer le workflow, car nous avons un mécanisme de fallback
          fi
          
          # Vérification des résultats
          cd ../../
          echo "Contenu du dossier scraping-results:"
          ls -la scraping-results/
          
          # Afficher le contenu du dernier fichier JSON généré (pour débogage)
          if [ "$DEBUG" = "true" ]; then
            LATEST_FILE=$(ls -t scraping-results/*.json | head -n1)
            if [ -n "$LATEST_FILE" ]; then
              echo "Aperçu du dernier fichier généré ($LATEST_FILE):"
              head -n 30 "$LATEST_FILE"
            else
              echo "Aucun fichier JSON trouvé dans le dossier scraping-results"
            fi
          fi
      
      - name: Archivage des résultats
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: scraping-results/
          retention-days: 7
      
      - name: Notification de réussite
        if: success()
        run: |
          echo "Scraping terminé avec succès"
          echo "Les résultats sont disponibles dans les artifacts du workflow"
      
      - name: Notification d'échec
        if: failure()
        run: |
          echo "Erreur lors du scraping"
          echo "Vérifiez les logs pour plus d'informations"
