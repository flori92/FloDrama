name: Pipeline de Scraping FloDrama

on:
  # Ex√©cution manuelle avec options configurables
  workflow_dispatch:
    inputs:
      skip_scraping:
        description: 'Sauter l''√©tape de scraping'
        required: false
        default: false
        type: boolean
      skip_enrichment:
        description: 'Sauter l''√©tape d''enrichissement'
        required: false
        default: false
        type: boolean
      skip_distribution:
        description: 'Sauter l''√©tape de distribution'
        required: false
        default: false
        type: boolean
      debug_mode:
        description: 'Activer le mode debug'
        required: false
        default: false
        type: boolean
      sources:
        description: 'Sources √† scraper (s√©par√©es par des virgules, vide = toutes)'
        required: false
        type: string
  
  # Ex√©cution planifi√©e (tous les jours √† 2h du matin)
  schedule:
    - cron: '0 2 * * *'
  
  # Ex√©cution lors d'un push sur les branches main et integration-cloudflare
  push:
    branches:
      - main
      - integration-cloudflare
    paths:
      - '.github/scripts/stealth/**'
      - '.github/workflows/scraping-pipeline-optimise.yml'
      - 'cloudflare/scraping/**'

# D√©finition des limites de temps et des permissions
concurrency:
  group: scraping-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scraping-pipeline:
    name: Ex√©cution du pipeline de scraping
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      DEBUG_MODE: ${{ github.event.inputs.debug_mode == 'true' }}
    
    steps:
      - name: Checkout du code
        uses: actions/checkout@v3
      
      - name: Configuration de Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Installation des d√©pendances
        run: |
          # D√©sactiver les scripts d'installation automatiques
          npm install --ignore-scripts
          npm install -g wrangler
          # Installation directe des d√©pendances n√©cessaires
          npm install --no-save fs-extra axios cheerio playwright playwright-extra playwright-extra-plugin-stealth
      
      - name: Installation des d√©pendances de Playwright
        run: |
          npx playwright install-deps
          npx playwright install chromium
      
      # √âtape de scraping
      - name: Ex√©cution du scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        id: scraping
        run: |
          echo "üîç D√©marrage du scraping..."
          
          # D√©terminer les sources √† scraper
          SOURCES_ARG=""
          if [ -n "${{ github.event.inputs.sources }}" ]; then
            SOURCES_ARG="--sources=${{ github.event.inputs.sources }}"
            echo "Sources sp√©cifi√©es: ${{ github.event.inputs.sources }}"
          else
            echo "Scraping de toutes les sources"
          fi
          
          # Ex√©cuter le script de scraping optimis√©
          node .github/scripts/stealth/scraper-optimise.js $SOURCES_ARG
          
          echo "‚úÖ Scraping termin√© avec succ√®s"
      
      # √âtape d'enrichissement des donn√©es
      - name: Enrichissement des donn√©es via TMDB
        if: ${{ github.event.inputs.skip_enrichment != 'true' && steps.scraping.outcome == 'success' }}
        run: |
          echo "üîç D√©marrage de l'enrichissement des donn√©es..."
          node .github/scripts/enrichment/tmdb-enricher.js
          echo "‚úÖ Enrichissement termin√© avec succ√®s"
      
      # √âtape de distribution des donn√©es
      - name: Distribution des donn√©es vers Cloudflare D1
        if: ${{ github.event.inputs.skip_distribution != 'true' && (steps.scraping.outcome == 'success' || github.event.inputs.skip_scraping == 'true') }}
        run: |
          echo "üì¶ D√©marrage de la distribution des donn√©es..."
          
          # Cr√©er la base de donn√©es D1 si elle n'existe pas
          wrangler d1 create flodrama-content --json || echo "Base de donn√©es existe d√©j√†"
          
          # Ex√©cuter les migrations de sch√©ma
          cd cloudflare/scraping
          wrangler d1 execute flodrama-content --file=./schema.sql
          
          # Importer les donn√©es
          node ./import-to-d1.js
          
          echo "‚úÖ Distribution termin√©e avec succ√®s"
      
      # Notification de r√©sultat
      - name: Notification Discord
        if: always()
        run: |
          STATUS="${{ job.status }}"
          COLOR="0xFF0000"  # Rouge par d√©faut (√©chec)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="0x00FF00"  # Vert (succ√®s)
            MESSAGE="‚úÖ Le pipeline de scraping s'est termin√© avec succ√®s!"
          else
            MESSAGE="‚ùå Le pipeline de scraping a √©chou√©!"
          fi
          
          # Envoyer la notification Discord
          curl -H "Content-Type: application/json" \
               -d "{\"embeds\": [{\"title\": \"FloDrama Scraping\", \"description\": \"$MESSAGE\", \"color\": $COLOR}]}" \
               $DISCORD_WEBHOOK
