name: Pipeline de Scraping FloDrama

on:
  # Ex√©cution manuelle avec options configurables
  workflow_dispatch:
    inputs:
      skip_scraping:
        description: 'Sauter l''√©tape de scraping'
        required: false
        default: false
        type: boolean
      skip_enrichment:
        description: 'Sauter l''√©tape d''enrichissement'
        required: false
        default: false
        type: boolean
      skip_distribution:
        description: 'Sauter l''√©tape de distribution'
        required: false
        default: false
        type: boolean
      debug_mode:
        description: 'Activer le mode debug'
        required: false
        default: false
        type: boolean
      use_relay_service:
        description: 'Utiliser le service relais Render pour le scraping'
        required: false
        default: true
        type: boolean
      sources:
        description: 'Sources √† scraper (s√©par√©es par des virgules, vide = toutes)'
        required: false
        type: string
  
  # Ex√©cution planifi√©e (tous les jours √† 2h du matin)
  schedule:
    - cron: '0 2 * * *'
  
  # Ex√©cution lors d'un push sur les branches main et integration-cloudflare
  push:
    branches:
      - main
      - integration-cloudflare
    paths:
      - '.github/scripts/stealth/**'
      - '.github/workflows/scraping-pipeline-optimise.yml'
      - 'cloudflare/scraping/**'

# D√©finition des limites de temps et des permissions
concurrency:
  group: scraping-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scraping-pipeline:
    name: Ex√©cution du pipeline de scraping
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      RENDER_API_KEY: ${{ secrets.RENDER_API_KEY || 'rnd_DJfpQC9gEu4KgTRvX8iQzMXxrteP' }}
      USE_RELAY_SERVICE: ${{ github.event.inputs.use_relay_service != 'false' }}
      DEBUG_MODE: ${{ github.event.inputs.debug_mode == 'true' }}
    
    steps:
      - name: Checkout du code
        uses: actions/checkout@v3
      
      - name: Configuration de Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Installation des d√©pendances
        run: |
          # D√©sactiver les scripts d'installation automatiques
          npm install --ignore-scripts
          npm install -g wrangler
          # Installation directe des d√©pendances n√©cessaires
          npm install --no-save fs-extra axios cheerio playwright playwright-extra playwright-extra-plugin-stealth
      
      - name: Installation des d√©pendances de Playwright
        run: |
          npx playwright install-deps
          npx playwright install chromium
      
      # V√©rification du service relais Render
      - name: V√©rification du service relais Render
        if: ${{ github.event.inputs.skip_scraping != 'true' && github.event.inputs.use_relay_service != 'false' }}
        id: check_relay
        run: |
          echo "üîÑ V√©rification de la disponibilit√© du service relais Render..."
          RELAY_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer $RENDER_API_KEY" https://flodrama-scraper.onrender.com/status)
          
          if [ "$RELAY_STATUS" = "200" ]; then
            echo "‚úÖ Service relais Render disponible"
            echo "use_relay_service=true" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Service relais Render indisponible (code: $RELAY_STATUS), utilisation du fallback local"
            echo "use_relay_service=false" >> $GITHUB_ENV
          fi
      
      # √âtape de scraping
      - name: Ex√©cution du scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        id: scraping
        run: |
          echo "üîç D√©marrage du scraping..."
          
          # D√©terminer les sources √† scraper
          SOURCES_ARG=""
          if [ -n "${{ github.event.inputs.sources }}" ]; then
            SOURCES_ARG="--sources=${{ github.event.inputs.sources }}"
            echo "Sources sp√©cifi√©es: ${{ github.event.inputs.sources }}"
          else
            echo "Scraping de toutes les sources"
          fi
          
          # Ex√©cuter le script de scraping optimis√©
          node .github/scripts/stealth/scraper-optimise.js $SOURCES_ARG
          
          # V√©rifier si des donn√©es ont √©t√© g√©n√©r√©es
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "‚úÖ Scraping termin√© avec succ√®s"
            echo "scraping_success=true" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Le scraping n'a pas g√©n√©r√© de donn√©es, v√©rifiez les logs pour plus d'informations"
            echo "scraping_success=false" >> $GITHUB_ENV
          fi
      
      # V√©rification du statut de scraping
      - name: V√©rification du statut de scraping
        id: check_scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        run: |
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "::set-output name=success::true"
          else
            echo "::set-output name=success::false"
          fi
      
      # √âtape d'enrichissement des donn√©es
      - name: Enrichissement des donn√©es via TMDB
        if: ${{ github.event.inputs.skip_enrichment != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true') }}
        run: |
          echo "üîç D√©marrage de l'enrichissement des donn√©es..."
          node .github/scripts/enrichment/tmdb-enricher.js
          echo "‚úÖ Enrichissement termin√© avec succ√®s"
      
      # √âtape de distribution des donn√©es
      - name: Distribution des donn√©es vers Cloudflare D1
        if: ${{ github.event.inputs.skip_distribution != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true' || github.event.inputs.skip_scraping == 'true') }}
        run: |
          echo "üì¶ D√©marrage de la distribution des donn√©es..."
          
          # Cr√©er la base de donn√©es D1 si elle n'existe pas
          wrangler d1 create flodrama-content --json || echo "Base de donn√©es existe d√©j√†"
          
          # Ex√©cuter les migrations de sch√©ma
          cd cloudflare/scraping
          wrangler d1 execute flodrama-content --file=./schema.sql
          
          # Importer les donn√©es
          node ./import-to-d1.js
          
          echo "‚úÖ Distribution termin√©e avec succ√®s"
      
      # Notification de r√©sultat
      - name: Notification Discord
        if: always()
        run: |
          STATUS="${{ job.status }}"
          COLOR="0xFF0000"  # Rouge par d√©faut (√©chec)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="0x00FF00"  # Vert (succ√®s)
            MESSAGE="‚úÖ Le pipeline de scraping s'est termin√© avec succ√®s!"
          else
            MESSAGE="‚ùå Le pipeline de scraping a √©chou√©!"
          fi
          
          # Ajouter des informations sur le mode de scraping utilis√©
          if [ "$USE_RELAY_SERVICE" == "true" ]; then
            SCRAPING_MODE="Service relais Render"
          else
            SCRAPING_MODE="Scraping local + fallback TMDB"
          fi
          
          # Envoyer la notification Discord avec plus de d√©tails
          curl -H "Content-Type: application/json" \
               -d "{\"embeds\": [{\"title\": \"FloDrama Scraping\", \"description\": \"$MESSAGE\", \"color\": $COLOR, \"fields\": [{\"name\": \"Mode de scraping\", \"value\": \"$SCRAPING_MODE\", \"inline\": true}, {\"name\": \"D√©clench√© par\", \"value\": \"${{ github.event_name }}\", \"inline\": true}, {\"name\": \"Date\", \"value\": \"$(date +\"%d/%m/%Y %H:%M\")\", \"inline\": true}]}]}" \
               $DISCORD_WEBHOOK
