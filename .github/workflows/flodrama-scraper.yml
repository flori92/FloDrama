name: Pipeline de Scraping FloDrama

on:
  # Ex√©cution manuelle avec options configurables
  workflow_dispatch:
    inputs:
      skip_scraping:
        description: 'Sauter l''√©tape de scraping'
        required: false
        default: false
        type: boolean
      skip_enrichment:
        description: 'Sauter l''√©tape d''enrichissement'
        required: false
        default: false
        type: boolean
      skip_distribution:
        description: 'Sauter l''√©tape de distribution'
        required: false
        default: false
        type: boolean
      debug_mode:
        description: 'Activer le mode debug'
        required: false
        default: false
        type: boolean
      use_relay_service:
        description: 'Utiliser le service relais Render pour le scraping'
        required: false
        default: true
        type: boolean
      sources:
        description: 'Sources √† scraper (s√©par√©es par des virgules, vide = toutes)'
        required: false
        type: string
      recent_only:
        description: 'Ne scraper que les contenus r√©cents (ann√©e courante - 2 ans)'
        required: false
        default: true
        type: boolean
  
  # Ex√©cution planifi√©e (tous les jours √† 2h du matin)
  schedule:
    - cron: '0 2 * * *'
  
  # Ex√©cution lors d'un push sur les branches main et integration-cloudflare
  push:
    branches:
      - main
      - integration-cloudflare
    paths:
      - '.github/scripts/stealth/**'
      - '.github/workflows/scraping-pipeline-optimise.yml'
      - 'cloudflare/scraping/**'

# D√©finition des limites de temps et des permissions
concurrency:
  group: scraping-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scraping-pipeline:
    name: Ex√©cution du pipeline de scraping
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    env:
      TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      RENDER_API_KEY: ${{ secrets.RENDER_API_KEY || 'rnd_DJfpQC9gEu4KgTRvX8iQzMXxrteP' }}
      USE_RELAY_SERVICE: ${{ github.event.inputs.use_relay_service != 'false' }}
      DEBUG_MODE: ${{ github.event.inputs.debug_mode == 'true' }}
    
    steps:
      - name: Checkout du code
        uses: actions/checkout@v3
      
      - name: Configuration de Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Installation des d√©pendances
        run: |
          # D√©sactiver les scripts d'installation automatiques
          npm install --ignore-scripts
          npm install -g wrangler
          # Installation directe des d√©pendances n√©cessaires
          npm install --no-save fs-extra axios cheerio playwright playwright-extra playwright-extra-plugin-stealth
      
      - name: Installation des d√©pendances de Playwright
        run: |
          npx playwright install-deps
          npx playwright install chromium
      
      # V√©rification du service relais Render
      - name: V√©rification du service relais Render
        if: ${{ github.event.inputs.skip_scraping != 'true' && github.event.inputs.use_relay_service != 'false' }}
        id: check_relay
        run: |
          echo "üîÑ V√©rification de la disponibilit√© du service relais Render..."
          RELAY_STATUS=$(curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer $RENDER_API_KEY" https://flodrama-scraper.onrender.com/status)
          
          if [ "$RELAY_STATUS" = "200" ]; then
            echo "‚úÖ Service relais Render disponible"
            echo "use_relay_service=true" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Service relais Render indisponible (code: $RELAY_STATUS), utilisation du fallback local"
            echo "use_relay_service=false" >> $GITHUB_ENV
          fi
      
      # √âtape de scraping
      - name: Ex√©cution du scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        id: scraping
        run: |
          echo "üîç D√©marrage du scraping..."
          
          # D√©terminer les sources √† scraper
          SOURCES_ARG=""
          if [ -n "${{ github.event.inputs.sources }}" ]; then
            SOURCES_ARG="--sources=${{ github.event.inputs.sources }}"
            echo "Sources sp√©cifi√©es: ${{ github.event.inputs.sources }}"
          else
            echo "Scraping de toutes les sources"
          fi
          
          # D√©terminer si on ne scrape que les contenus r√©cents
          RECENT_ONLY="${{ github.event.inputs.recent_only != 'false' }}"
          if [ "$RECENT_ONLY" = "true" ]; then
            # Calculer la plage d'ann√©es dynamique (ann√©e courante - 2 ans)
            CURRENT_YEAR=$(date +"%Y")
            MIN_YEAR=$((CURRENT_YEAR - 2))
            echo "Filtrage activ√©: uniquement les contenus entre $MIN_YEAR et $CURRENT_YEAR"
            YEARS_ARG="--years=$MIN_YEAR,$CURRENT_YEAR"
          else
            echo "Aucun filtrage par ann√©e: tous les contenus seront scrap√©s"
            YEARS_ARG=""
          fi
          
          # Ex√©cuter le script de scraping optimis√©
          node .github/scripts/stealth/scraper-optimise.js $SOURCES_ARG $YEARS_ARG
          
          # Si nous utilisons Cloudflare Workers pour le scraping, lancer √©galement la version Cloudflare
          if [ "$RECENT_ONLY" = "true" ]; then
            echo "üîç Lancement du scraping via Cloudflare Workers..."
            # Mettre √† jour les services Cloudflare avec la plage d'ann√©es dynamique
            cd cloudflare/recommendations
            
            # Mettre √† jour le service de recommandation pour qu'il utilise la plage d'ann√©es dynamique
            sed -i "s/WHERE release_year BETWEEN [0-9]\{4\} AND [0-9]\{4\}/WHERE release_year BETWEEN $MIN_YEAR AND $CURRENT_YEAR/g" src/services/recommendation-service.js
            
            # D√©ployer les modifications
            ./deploy.sh
            
            # Lancer le scraping pour chaque source via l'API Cloudflare
            API_KEY=$(grep -o 'API Key: [a-f0-9]\{32\}' deploy.log | cut -d' ' -f3)
            
            if [ -n "$API_KEY" ]; then
              echo "Lancement du scraping via l'API Cloudflare avec la cl√©: $API_KEY"
              
              # Liste des sources √† scraper
              if [ -n "${{ github.event.inputs.sources }}" ]; then
                IFS=',' read -ra SOURCES <<< "${{ github.event.inputs.sources }}"
              else
                SOURCES=("kissasian" "dramacool" "viewasian" "voirdrama" "gogoanime" "nekosama" "voiranime" "vostfree" "streamingdivx" "filmcomplet" "bollyplay" "hindilinks4u")
              fi
              
              for SOURCE in "${SOURCES[@]}"; do
                echo "Scraping de la source: $SOURCE"
                curl -X POST https://flodrama-recommendations-prod.florifavi.workers.dev/api/scrape \
                  -H "X-API-Key: $API_KEY" \
                  -H "Content-Type: application/json" \
                  -d '{"source_id": "'$SOURCE'", "target_years": ['$MIN_YEAR', '$CURRENT_YEAR']}'
                
                # Attendre 5 secondes entre chaque source
                sleep 5
              done
            else
              echo "Impossible de r√©cup√©rer la cl√© API, le scraping via Cloudflare n'a pas √©t√© lanc√©"
            fi
            
            cd ../..
          fi
          
          # V√©rifier si des donn√©es ont √©t√© g√©n√©r√©es
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "‚úÖ Scraping termin√© avec succ√®s"
            echo "scraping_success=true" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è Le scraping n'a pas g√©n√©r√© de donn√©es, v√©rifiez les logs pour plus d'informations"
            echo "scraping_success=false" >> $GITHUB_ENV
          fi
      
      # V√©rification du statut de scraping
      - name: V√©rification du statut de scraping
        id: check_scraping
        if: ${{ github.event.inputs.skip_scraping != 'true' }}
        run: |
          if [ -d "./cloudflare/scraping/output" ] && [ "$(find ./cloudflare/scraping/output -name '*.json' | wc -l)" -gt 0 ]; then
            echo "::set-output name=success::true"
          else
            echo "::set-output name=success::false"
          fi
      
      # √âtape d'enrichissement des donn√©es
      - name: Enrichissement des donn√©es via TMDB
        if: ${{ github.event.inputs.skip_enrichment != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true') }}
        run: |
          echo "üîç D√©marrage de l'enrichissement des donn√©es..."
          node .github/scripts/enrichment/tmdb-enricher.js
          echo "‚úÖ Enrichissement termin√© avec succ√®s"
      
      # √âtape de distribution des donn√©es
      - name: Distribution des donn√©es vers Cloudflare D1
        if: ${{ github.event.inputs.skip_distribution != 'true' && (steps.scraping.outcome == 'success' || steps.check_scraping.outputs.success == 'true' || github.event.inputs.skip_scraping == 'true') }}
        run: |
          echo "üì¶ D√©marrage de la distribution des donn√©es..."
          
          # Cr√©er la base de donn√©es D1 si elle n'existe pas
          wrangler d1 create flodrama-content --json || echo "Base de donn√©es existe d√©j√†"
          
          # Ex√©cuter les migrations de sch√©ma
          cd cloudflare/scraping
          wrangler d1 execute flodrama-content --file=./schema.sql
          
          # D√©terminer si on ne distribue que les contenus r√©cents
          RECENT_ONLY="${{ github.event.inputs.recent_only != 'false' }}"
          if [ "$RECENT_ONLY" = "true" ]; then
            # Calculer la plage d'ann√©es dynamique (ann√©e courante - 2 ans)
            CURRENT_YEAR=$(date +"%Y")
            MIN_YEAR=$((CURRENT_YEAR - 2))
            echo "Filtrage activ√©: uniquement les contenus entre $MIN_YEAR et $CURRENT_YEAR"
            
            # Supprimer les contenus anciens de la base de donn√©es
            echo "Suppression des contenus ant√©rieurs √† $MIN_YEAR..."
            wrangler d1 execute flodrama-content --command="DELETE FROM contents WHERE release_year < $MIN_YEAR"
            
            # Importer les donn√©es avec le filtre d'ann√©e
            echo "Importation des donn√©es r√©centes uniquement..."
            node ./import-to-d1.js --min-year=$MIN_YEAR
          else
            # Importer toutes les donn√©es sans filtre
            echo "Importation de toutes les donn√©es sans filtre d'ann√©e..."
            node ./import-to-d1.js
          fi
          
          # Synchroniser avec la base de donn√©es de recommandation
          echo "Synchronisation avec la base de donn√©es de recommandation..."
          cd ../recommendations
          
          # D√©ployer les modifications si n√©cessaire
          if [ "$RECENT_ONLY" = "true" ]; then
            # Mettre √† jour le service de recommandation pour qu'il utilise la plage d'ann√©es dynamique
            CURRENT_YEAR=$(date +"%Y")
            MIN_YEAR=$((CURRENT_YEAR - 2))
            
            # V√©rifier si le service a d√©j√† √©t√© mis √† jour dans l'√©tape de scraping
            if ! grep -q "WHERE release_year BETWEEN $MIN_YEAR AND $CURRENT_YEAR" src/services/recommendation-service.js; then
              sed -i "s/WHERE release_year BETWEEN [0-9]\{4\} AND [0-9]\{4\}/WHERE release_year BETWEEN $MIN_YEAR AND $CURRENT_YEAR/g" src/services/recommendation-service.js
              ./deploy.sh
            fi
          fi
          
          cd ../..
          
          echo "‚úÖ Distribution termin√©e avec succ√®s"
      
      # Notification de r√©sultat
      - name: Notification Discord
        if: always()
        run: |
          STATUS="${{ job.status }}"
          COLOR="0xFF0000"  # Rouge par d√©faut (√©chec)
          
          if [ "$STATUS" == "success" ]; then
            COLOR="0x00FF00"  # Vert (succ√®s)
            MESSAGE="‚úÖ Le pipeline de scraping s'est termin√© avec succ√®s!"
          else
            MESSAGE="‚ùå Le pipeline de scraping a √©chou√©!"
          fi
          
          # Ajouter des informations sur le mode de scraping utilis√©
          if [ "$USE_RELAY_SERVICE" == "true" ]; then
            SCRAPING_MODE="Service relais Render"
          else
            SCRAPING_MODE="Scraping local + fallback TMDB"
          fi
          
          # Envoyer la notification Discord avec plus de d√©tails
          curl -H "Content-Type: application/json" \
               -d "{\"embeds\": [{\"title\": \"FloDrama Scraping\", \"description\": \"$MESSAGE\", \"color\": $COLOR, \"fields\": [{\"name\": \"Mode de scraping\", \"value\": \"$SCRAPING_MODE\", \"inline\": true}, {\"name\": \"D√©clench√© par\", \"value\": \"${{ github.event_name }}\", \"inline\": true}, {\"name\": \"Date\", \"value\": \"$(date +\"%d/%m/%Y %H:%M\")\", \"inline\": true}]}]}" \
               $DISCORD_WEBHOOK
