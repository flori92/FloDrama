/**
 * Syst√®me de scraping avanc√© pour FloDrama bas√© sur Crawlee
 * 
 * Ce script utilise la biblioth√®que Crawlee pour extraire des milliers de contenus
 * depuis diverses sources populaires et les rendre disponibles pour FloDrama.
 * 
 * Caract√©ristiques:
 * - Scraping parall√®le de multiples sources
 * - Gestion intelligente du cache
 * - Contournement des protections anti-bot
 * - Extraction bas√©e sur l'IA pour s'adapter aux changements de structure
 * - Support pour les sites n√©cessitant JavaScript
 */

const fs = require('fs-extra');
const path = require('path');
const { PlaywrightCrawler, CheerioCrawler, Dataset } = require('crawlee');
const cheerio = require('cheerio');

// Importer les modules personnalis√©s
const { CONFIG, SOURCE_CONFIG } = require('./crawlee/config');
const { checkCache, saveData, chunkArray, formatDuration } = require('./crawlee/utils');
const { getExtractor } = require('./crawlee/extractors');

// Statistiques globales
const stats = {
  total_items: 0,
  sources_processed: 0,
  sources_failed: 0,
  start_time: new Date(),
  end_time: null,
  duration_ms: 0,
  duration_formatted: '',
  categories: {},
  sources: {}
};

/**
 * Fonction principale
 */
async function main() {
  console.log('='.repeat(80));
  console.log(`FloDrama - Syst√®me de scraping avanc√© bas√© sur Crawlee`);
  console.log('='.repeat(80));
  
  console.log(`\nConfiguration:`);
  console.log(`- Sources √† scraper: ${CONFIG.SOURCES.length} (${CONFIG.SOURCES.join(', ')})`);
  console.log(`- Minimum d'√©l√©ments par source: ${CONFIG.MIN_ITEMS_PER_SOURCE}`);
  console.log(`- Scraping parall√®le: ${CONFIG.PARALLEL_SCRAPING ? 'Oui' : 'Non'}`);
  console.log(`- Nombre maximum de sources en parall√®le: ${CONFIG.MAX_CONCURRENT_SOURCES}`);
  console.log(`- Tentatives de retry: ${CONFIG.MAX_RETRIES}`);
  console.log(`- TTL du cache: ${Math.round(CONFIG.CACHE_TTL / 60000)} minutes`);
  
  // Cr√©er les r√©pertoires n√©cessaires
  await fs.ensureDir(CONFIG.OUTPUT_DIR);
  await fs.ensureDir(CONFIG.CACHE_DIR);
  await fs.ensureDir(path.join(CONFIG.OUTPUT_DIR, 'logs'));
  
  // Filtrer les sources configur√©es
  const sourcesToScrape = CONFIG.SOURCES.filter(source => SOURCE_CONFIG[source]);
  
  if (sourcesToScrape.length === 0) {
    console.error('Aucune source configur√©e √† scraper!');
    return;
  }
  
  console.log(`\nüîç D√©marrage du scraping pour ${sourcesToScrape.length} sources...`);
  
  // Scraper les sources (en parall√®le ou s√©quentiellement)
  if (CONFIG.PARALLEL_SCRAPING) {
    const sourceChunks = chunkArray(sourcesToScrape, CONFIG.MAX_CONCURRENT_SOURCES);
    
    for (const [chunkIndex, chunk] of sourceChunks.entries()) {
      console.log(`\nüì¶ Traitement du lot ${chunkIndex + 1}/${sourceChunks.length} (${chunk.length} sources)`);
      
      await Promise.all(chunk.map(source => scrapeSource(source)));
    }
  } else {
    for (const source of sourcesToScrape) {
      await scrapeSource(source);
    }
  }
  
  // Calculer la dur√©e totale
  stats.end_time = new Date();
  stats.duration_ms = stats.end_time - stats.start_time;
  stats.duration_formatted = formatDuration(stats.duration_ms);
  
  // Afficher les statistiques d√©taill√©es
  console.log('\nüìä Statistiques du scraping:');
  console.log(`‚è±Ô∏è Dur√©e totale: ${stats.duration_formatted}`);
  console.log(`üì¶ Total d'√©l√©ments: ${stats.total_items}`);
  console.log(`‚úÖ Sources trait√©es: ${stats.sources_processed}/${sourcesToScrape.length}`);
  console.log(`‚ùå Sources en √©chec: ${stats.sources_failed}`);
  
  // Afficher les statistiques par cat√©gorie
  console.log('\nüìÇ Statistiques par cat√©gorie:');
  for (const [category, count] of Object.entries(stats.categories)) {
    console.log(`- ${category}: ${count} √©l√©ments`);
  }
  
  // Sauvegarder les statistiques
  await fs.writeJson(
    path.join(CONFIG.OUTPUT_DIR, 'logs', 'crawlee-stats.json'),
    stats,
    { spaces: 2 }
  );
  
  console.log('\n‚ú® Scraping termin√© avec succ√®s!');
}

/**
 * Scrape une source sp√©cifique
 * @param {string} sourceName - Nom de la source √† scraper
 */
async function scrapeSource(sourceName) {
  console.log(`\nüîç Scraping de ${sourceName}...`);
  
  try {
    const config = SOURCE_CONFIG[sourceName];
    
    if (!config) {
      throw new Error(`Configuration manquante pour la source: ${sourceName}`);
    }
    
    // V√©rifier le cache
    const { useCache, cachedData } = await checkCache(sourceName);
    
    if (useCache) {
      console.log(`[${sourceName}] Utilisation des donn√©es en cache (${cachedData.length} √©l√©ments)`);
      
      // Mettre √† jour les statistiques
      stats.total_items += cachedData.length;
      stats.sources_processed++;
      
      // Mettre √† jour les statistiques par cat√©gorie
      const category = config.type || 'unknown';
      stats.categories[category] = (stats.categories[category] || 0) + cachedData.length;
      
      // Mettre √† jour les statistiques par source
      stats.sources[sourceName] = {
        items: cachedData.length,
        success: true,
        cached: true
      };
      
      return cachedData;
    }
    
    // Scraper les donn√©es en direct
    console.log(`[${sourceName}] Scraping en direct...`);
    
    // S√©lectionner le type de crawler appropri√©
    const items = await (config.extractorType === 'playwright' 
      ? scrapeWithPlaywright(sourceName, config)
      : scrapeWithCheerio(sourceName, config));
    
    if (!items || items.length === 0) {
      throw new Error(`Aucun √©l√©ment r√©cup√©r√© depuis ${sourceName}`);
    }
    
    console.log(`[${sourceName}] ${items.length} √©l√©ments r√©cup√©r√©s`);
    
    // V√©rifier si le nombre d'√©l√©ments est suffisant
    if (items.length < CONFIG.MIN_ITEMS_PER_SOURCE) {
      console.warn(`[${sourceName}] Attention: Nombre d'√©l√©ments insuffisant (${items.length}/${CONFIG.MIN_ITEMS_PER_SOURCE})`);
      
      // Si nous avons des donn√©es en cache, les utiliser comme fallback
      if (cachedData && cachedData.length > items.length) {
        console.log(`[${sourceName}] Utilisation des donn√©es en cache comme fallback (${cachedData.length} √©l√©ments)`);
        
        // Mettre √† jour les statistiques
        stats.total_items += cachedData.length;
        stats.sources_processed++;
        
        // Mettre √† jour les statistiques par cat√©gorie
        const category = config.type || 'unknown';
        stats.categories[category] = (stats.categories[category] || 0) + cachedData.length;
        
        // Mettre √† jour les statistiques par source
        stats.sources[sourceName] = {
          items: cachedData.length,
          success: true,
          cached: true,
          fallback: true
        };
        
        // Sauvegarder les donn√©es
        await saveData(sourceName, cachedData);
        
        return cachedData;
      }
    }
    
    // Sauvegarder les donn√©es
    await saveData(sourceName, items);
    
    // Mettre √† jour les statistiques
    stats.total_items += items.length;
    stats.sources_processed++;
    
    // Mettre √† jour les statistiques par cat√©gorie
    const category = config.type || 'unknown';
    stats.categories[category] = (stats.categories[category] || 0) + items.length;
    
    // Mettre √† jour les statistiques par source
    stats.sources[sourceName] = {
      items: items.length,
      success: true,
      cached: false
    };
    
    return items;
  } catch (error) {
    console.error(`[${sourceName}] Erreur: ${error.message}`);
    stats.sources_failed++;
    
    // Mettre √† jour les statistiques par source
    stats.sources[sourceName] = {
      items: 0,
      success: false,
      error: error.message
    };
    
    return [];
  }
}

/**
 * Scrape une source avec Playwright (pour les sites n√©cessitant JavaScript)
 * @param {string} sourceName - Nom de la source
 * @param {Object} config - Configuration de la source
 * @returns {Promise<Array>} - √âl√©ments r√©cup√©r√©s
 */
async function scrapeWithPlaywright(sourceName, config) {
  console.log(`[${sourceName}] Utilisation de Playwright pour le scraping...`);
  
  // Cr√©er un dataset pour stocker les r√©sultats
  const datasetName = sourceName;
  await Dataset.open(datasetName);
  
  // R√©cup√©rer l'extracteur appropri√©
  const extractor = getExtractor(sourceName);
  
  // Initialiser le crawler
  const crawler = new PlaywrightCrawler({
    maxRequestsPerCrawl: config.pagination?.maxPages || 1,
    maxRequestRetries: CONFIG.MAX_RETRIES,
    
    // Configuration avanc√©e pour √©viter la d√©tection
    launchContext: {
      launchOptions: {
        headless: true,
        args: [
          '--disable-dev-shm-usage',
          '--disable-setuid-sandbox',
          '--no-sandbox',
          '--disable-gpu'
        ]
      }
    },
    
    // Fonction d'extraction
    async requestHandler({ page, request, enqueueLinks }) {
      console.log(`[${sourceName}] Scraping de ${request.url}`);
      
      // Simuler un comportement humain
      await page.setViewportSize({ width: 1366, height: 768 });
      await page.setExtraHTTPHeaders(CONFIG.BROWSER_HEADERS);
      await page.waitForTimeout(Math.random() * 2000 + 1000);
      
      // Attendre que le contenu soit charg√©
      if (config.waitForSelector) {
        await page.waitForSelector(config.selector || config.waitForSelector);
      } else {
        await page.waitForLoadState('networkidle');
      }
      
      // Extraction des donn√©es
      const items = await page.evaluate((selector, sourceName, configStr) => {
        const config = JSON.parse(configStr);
        const items = [];
        
        // S√©lectionner tous les √©l√©ments correspondants
        document.querySelectorAll(selector).forEach((element, index) => {
          try {
            // Extraire les donn√©es de base
            const link = element.querySelector('a');
            const img = element.querySelector('img');
            const title = element.querySelector('h2, h3, .title, .name') || link;
            
            if (!link || !title) return;
            
            // Extraire l'URL et l'ID
            const url = link.href;
            const id = url ? url.split('/').pop() : `${sourceName}_${index}`;
            
            // Extraire le titre
            const titleText = title.innerText?.trim() || link.getAttribute('title') || '';
            
            // Extraire l'image
            const poster = img?.src || img?.getAttribute('data-src') || img?.getAttribute('data-original') || '';
            
            // Extraire l'ann√©e et la note
            const fullText = element.innerText || '';
            const yearMatch = fullText.match(/\b(20\d{2}|19\d{2})\b/);
            const year = yearMatch ? parseInt(yearMatch[1]) : new Date().getFullYear();
            
            const ratingMatch = fullText.match(/([0-9](\.[0-9])?|10(\.0)?)\s*\/\s*10/) || 
                                fullText.match(/([0-9](\.[0-9])?|10(\.0)?)/);
            const rating = ratingMatch ? parseFloat(ratingMatch[1]) : 0;
            
            // Cr√©er l'objet item
            items.push({
              id: `${sourceName}_${id}`,
              title: titleText,
              original_title: titleText,
              url: url,
              poster: poster,
              backdrop: poster,
              year: year,
              rating: rating,
              source: sourceName,
              type: config.type || 'unknown',
              language: config.language || 'multi',
              created_at: new Date().toISOString(),
              updated_at: new Date().toISOString()
            });
          } catch (error) {
            console.error(`Erreur lors de l'extraction: ${error.message}`);
          }
        });
        
        return items;
      }, config.selector, sourceName, JSON.stringify(config));
      
      // Sauvegarder les donn√©es dans le dataset
      await Dataset.pushData(items);
      
      // Si la pagination est activ√©e, ajouter les pages suivantes √† la file d'attente
      if (config.pagination?.enabled && request.userData.page < (config.pagination.maxPages || 1)) {
        const nextPage = (request.userData.page || 1) + 1;
        const nextUrl = `${config.pagination.baseUrl}${nextPage}`;
        
        await crawler.addRequests([{
          url: nextUrl,
          userData: { page: nextPage }
        }]);
      }
    },
    
    // Gestion des erreurs
    failedRequestHandler({ request, error }) {
      console.error(`[${sourceName}] Erreur lors du scraping de ${request.url}: ${error.message}`);
    }
  });
  
  // D√©marrer le crawling
  await crawler.run([{
    url: config.url,
    userData: { page: 1 }
  }]);
  
  // R√©cup√©rer les donn√©es
  const dataset = await Dataset.open(datasetName);
  const results = await dataset.getData();
  
  return results.items.flat();
}

/**
 * Scrape une source avec Cheerio (pour les sites statiques)
 * @param {string} sourceName - Nom de la source
 * @param {Object} config - Configuration de la source
 * @returns {Promise<Array>} - √âl√©ments r√©cup√©r√©s
 */
async function scrapeWithCheerio(sourceName, config) {
  console.log(`[${sourceName}] Utilisation de Cheerio pour le scraping...`);
  
  // Cr√©er un dataset pour stocker les r√©sultats
  const datasetName = sourceName;
  await Dataset.open(datasetName);
  
  // R√©cup√©rer l'extracteur appropri√©
  const extractor = getExtractor(sourceName);
  
  // Initialiser le crawler
  const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: config.pagination?.maxPages || 1,
    maxRequestRetries: CONFIG.MAX_RETRIES,
    
    // Configuration pour √©viter la d√©tection
    headerGeneratorOptions: {
      browsers: [
        { name: 'chrome', minVersion: 87, maxVersion: 89 },
        { name: 'firefox', minVersion: 84 }
      ],
      devices: ['desktop'],
      locales: ['fr-FR', 'en-US']
    },
    
    // Fonction d'extraction
    async requestHandler({ $, request, enqueueLinks }) {
      console.log(`[${sourceName}] Scraping de ${request.url}`);
      
      // S√©lectionner tous les √©l√©ments correspondants
      const elements = $(config.selector);
      console.log(`[${sourceName}] ${elements.length} √©l√©ments trouv√©s sur la page`);
      
      const items = [];
      
      // Extraire les donn√©es de chaque √©l√©ment
      elements.each((index, element) => {
        try {
          const item = extractor($, element, sourceName, config);
          if (item) {
            items.push(item);
          }
        } catch (error) {
          console.error(`[${sourceName}] Erreur lors de l'extraction: ${error.message}`);
        }
      });
      
      // Sauvegarder les donn√©es dans le dataset
      await Dataset.pushData(items);
      
      // Si la pagination est activ√©e, ajouter les pages suivantes √† la file d'attente
      if (config.pagination?.enabled && request.userData.page < (config.pagination.maxPages || 1)) {
        const nextPage = (request.userData.page || 1) + 1;
        const nextUrl = `${config.pagination.baseUrl}${nextPage}`;
        
        await crawler.addRequests([{
          url: nextUrl,
          userData: { page: nextPage }
        }]);
      }
    },
    
    // Gestion des erreurs
    failedRequestHandler({ request, error }) {
      console.error(`[${sourceName}] Erreur lors du scraping de ${request.url}: ${error.message}`);
    }
  });
  
  // D√©marrer le crawling
  await crawler.run([{
    url: config.url,
    userData: { page: 1 }
  }]);
  
  // R√©cup√©rer les donn√©es
  const dataset = await Dataset.open(datasetName);
  const results = await dataset.getData();
  
  return results.items.flat();
}

// Ex√©cuter la fonction principale
main()
  .then(() => process.exit(0))
  .catch(error => {
    console.error('Erreur fatale:', error);
    process.exit(1);
  });
