#!/bin/bash
# Script pour ex√©cuter le scraping et alimenter Supabase

# Variables de configuration
export SUPABASE_URL="https://fffgoqubrbgppcqqkyod.supabase.co"
export SUPABASE_SERVICE_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZmZmdvcXVicmJncHBjcXFreW9kIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc0NTY4MzUwNCwiZXhwIjoyMDYxMjU5NTA0fQ.0ahlAEbmf3eK-utoUTuBEFSOpO_2qPN6k_YdxQzL4XI"
export MIN_ITEMS="100"

# Cr√©ation du dossier de logs
mkdir -p scraping/logs

# Cr√©ation d'un log pour le scraping
LOG_FILE="scraping/logs/scraping-$(date +%Y%m%d-%H%M%S).log"
echo "üìù Les logs seront enregistr√©s dans $LOG_FILE"

# V√©rification des d√©pendances Python
echo "üîç V√©rification des d√©pendances Python..."
pip install supabase requests beautifulsoup4 python-dotenv

# Ex√©cution de la migration SQL
echo "üîß Ex√©cution de la migration SQL pour corriger la table scraping_logs..."
python supabase/scripts/execute_migration.py 2>&1 | tee -a "$LOG_FILE"

# Ex√©cution du scraping unifi√© pour toutes les cat√©gories
echo "üöÄ Lancement du scraping unifi√© pour toutes les cat√©gories..."
cd "$(dirname "$0")/.." && python -m scraping.unified_scraper --all 2>&1 | tee -a "$LOG_FILE"

echo "‚úÖ Scraping termin√© ! V√©rifiez les logs pour plus de d√©tails."
